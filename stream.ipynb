{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stream.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwtiqjlSfok6"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "\n",
        "pat1 = r'@[A-Za-z0-9_]+' # For removing mentions.\n",
        "pat2 = r'https?://[^ ]+' # For removing links.\n",
        "combined_pat = r'|'.join((pat1, pat2)) # For removing links.\n",
        "www_pat = r'www.[^ ]+' # For removing links.\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"} # For removing negations.\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b') # For removing negations.\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "      \"\"\" This cleans the tweets text content\n",
        "          so it can be machine readable.\n",
        "      \"\"\"\n",
        "      soup = BeautifulSoup(text, 'lxml') # Decodes HTML.\n",
        "      souped = soup.get_text()\n",
        "      try: # UTF decoding.\n",
        "          bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "      except:\n",
        "          bom_removed = souped\n",
        "      stripped = re.sub(combined_pat, '', bom_removed) # Gets rid of links.\n",
        "      stripped = re.sub(www_pat, '', stripped) # Gets rid of links.\n",
        "      lower_case = stripped.lower()\n",
        "      neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
        "      letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled) # Only lets letters in.\n",
        "      words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Below removes whitespace\n",
        "      return (\" \".join(words)).strip()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VDuWSZNo5af"
      },
      "source": [
        "import tweepy\n",
        "import datetime\n",
        "import time\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "def main(ticker, name, timer):\n",
        "    \"\"\" timer - minutes \"\"\"\n",
        "\n",
        "    def writer(f, text, status):\n",
        "        \"\"\"Writes items to file in proper format \"\"\"\n",
        "        text = tweet_cleaner(text)\n",
        "        analyser = SentimentIntensityAnalyzer()\n",
        "        score = analyser.polarity_scores(text)\n",
        "        pos, compound, neu, neg = score[\"pos\"], score[\"compound\"], score[\"neu\"], score[\"neg\"]\n",
        "        f.write(str(status.favorite_count)+\",\"+str(status.retweet_count)+\",\"+str(status.source)+\",\"+\"\\\"\"+str(tweet_cleaner(status.user.location))+\"\\\"\"+\",\"+str(status.created_at)+\",\"+text+\",\"+str(status.id)+\",\"+str(pos)+\",\"+str(compound)+\",\"+str(neu)+\",\"+str(neg)+\"\\n\")\n",
        "\n",
        "    class MyStreamListener(tweepy.StreamListener):\n",
        "        def __init__(self, api):\n",
        "            self.api = api\n",
        "            self.me = api.me()\n",
        "\n",
        "        def on_status(self, status):\n",
        "            \"\"\"This Status event handler for a StreamListener prints the full text of the Tweet,\n",
        "               or if itâ€™s a Retweet, the full text of the Retweeted Tweet.\n",
        "               https://www.geeksforgeeks.org/python-status-object-in-tweepy/ - Attributes\n",
        "            \"\"\"\n",
        "            # If status is a Retweet, it will not have an extended_tweet attribute, and status.text could be truncated.\n",
        "            if hasattr(status, \"retweeted_status\"):  # Check if Retweet\n",
        "                try:\n",
        "                    writer(f, status.retweeted_status.extended_tweet[\"full_text\"], status)\n",
        "                except AttributeError:\n",
        "                    writer(f, status.retweeted_status.text, status)\n",
        "            else:\n",
        "                try:\n",
        "                    writer(f, status.extended_tweet[\"full_text\"], status)\n",
        "                except AttributeError:\n",
        "                    writer(f, status.text, status)\n",
        "\n",
        "            if time.time() > timeout: # Exits after x seconds has passed.\n",
        "                return False\n",
        "\n",
        "        def on_error(self, status):\n",
        "            print(\"Error detected\")\n",
        "\n",
        "    # Sets timer.\n",
        "    timeout = time.time() + timer*60\n",
        "\n",
        "    # Set up file.\n",
        "    start_time = datetime.datetime.now()\n",
        "    end_time = start_time + datetime.timedelta(minutes=timer)\n",
        "    start_time = start_time.strftime(\"%d:%m:%y %H:%M\")\n",
        "    end_time = end_time.strftime(\"%d:%m:%y %H:%M\")\n",
        "    f = open(f\"{ticker}: {start_time} - {end_time}.csv\", \"w+\")\n",
        "    f.write(\"Like count, Retweet count, Source, User location, date, tweet, tweet_id, pos, compound, neu, neg\\n\")\n",
        "\n",
        "    # Authenticate to Twitter\n",
        "    auth = tweepy.OAuthHandler(api_key, api_secret)\n",
        "    auth.set_access_token(access_token, access_secret)\n",
        "\n",
        "    # Create API object\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "    tweets_listener = MyStreamListener(api)\n",
        "    stream = tweepy.Stream(api.auth, tweets_listener)\n",
        "    stream.filter(track=[ticker, name], languages=[\"en\"])\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}