{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stream.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "knhtsonBdyLY"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "\n",
        "pat1 = r'@[A-Za-z0-9_]+' # For removing mentions.\n",
        "pat2 = r'https?://[^ ]+' # For removing links.\n",
        "combined_pat = r'|'.join((pat1, pat2)) # For removing links.\n",
        "www_pat = r'www.[^ ]+' # For removing links.\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"} # For removing negations.\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b') # For removing negations.\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "  \"\"\" This cleans the tweets text content \n",
        "      so it can be machine readable.\n",
        "  \"\"\"\n",
        "  soup = BeautifulSoup(text, 'lxml') # Decodes HTML.\n",
        "  souped = soup.get_text()\n",
        "  try: # UTF decoding.\n",
        "    bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "  except:\n",
        "    bom_removed = souped\n",
        "  stripped = re.sub(combined_pat, '', bom_removed) # Gets rid of links.\n",
        "  stripped = re.sub(www_pat, '', stripped) # Gets rid of links.\n",
        "  lower_case = stripped.lower()\n",
        "  neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
        "  letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled) # Only lets letters in.\n",
        "  words = [x for x  in tok.tokenize(letters_only) if len(x) > 1] # Below removes whitespace\n",
        "  return (\" \".join(words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwtiqjlSfok6"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "#from api_info import * # api_key, api_secret, access_token, access_secret\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "ticker, name = \"TSLA\", \"Tesla\" # In the application this should be given as an input.\n",
        "timeout = 5 # In seconds\n",
        "\n",
        "endtime = datetime.datetime.now() + datetime.timedelta(seconds = timeout) # Sets timer.\n",
        "f = open(f\"{ticker}.csv\", \"a\", encoding='utf-8') # Appends to file if it exists, creates if it doesn't.\n",
        "if os.stat(f\"{ticker}.csv\").st_size == 0: # Creates title if file is empty.\n",
        "  f.write(\"Time, Text\\n\")\n",
        "\n",
        "class listener(StreamListener):\n",
        "  \"\"\" This class outputs any tweet based on a keyword as soon\n",
        "      as it's posted.\n",
        "  \"\"\"\n",
        "  def on_data(self, data):\n",
        "    tweet = json.loads(data) # Converts  string to dictionary.\n",
        "\n",
        "    text = tweet_cleaner(tweet[\"text\"])\n",
        "    time = str(datetime.datetime.now())\n",
        "  \n",
        "    f.write(time + \", \" + text + \"\\n\") # Outputs information to file.\n",
        "    f.flush() # IDK what this is if I don't put it here it doesn't work.\n",
        "\n",
        "    if datetime.datetime.now() > endtime:\n",
        "      print(\"it works\")\n",
        "      f.close()\n",
        "      exit() # This line only works in .py files, not notebooks (it crashes)\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status) # Prints the error if it exists.\n",
        "\n",
        "auth = OAuthHandler(api_key, api_secret) # Initializes API.\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "\n",
        "twitterStream = Stream(auth, listener(), tweet_mode=\"extended\") # Calls listener to run.\n",
        "twitterStream.filter(languages=[\"en\"], track=[ticker, name])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}